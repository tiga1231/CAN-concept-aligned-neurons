{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cae31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SpaCy uses nltk to handle wordnet\n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('brown')\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "line_break = '-'*40\n",
    "big_line_break = '='*40\n",
    "# word_frequency = nltk.FreqDist(w.lower() for w in brown.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e9f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordfreq\n",
    "from wordfreq import word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install mysql-client\n",
    "# sudo apt-get install mysql-server\n",
    "# sudo apt-get install libmysqlclient-dev\n",
    "# !pip install mysqlclient==2.1.1 pattern\n",
    "\n",
    "from pattern.en import singularize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e56466",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load neuron-concept similarity for ranking words\n",
    "similarities = torch.cat(\n",
    "    [s[\"similarities\"] for s in torch.load(\"my_data/all_layer_similarities.pt\")]\n",
    ")\n",
    "\n",
    "with open(\"data/20k.txt\") as f:\n",
    "    vocabulary = [l.strip() for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17db695",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sum = similarities.clip(0,torch.inf).max(0).values\n",
    "\n",
    "plt.hist(sim_sum, bins=100)\n",
    "sim_sum.shape, sim_sum.quantile(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "argsort = sim_sum.argsort(descending=True)\n",
    "# [vocabulary[i] for i in argsort[:30]]\n",
    "\n",
    "print('bottom 30:')\n",
    "[vocabulary[i] for i in argsort[-30:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b98174",
   "metadata": {},
   "source": [
    "## Word list used to grab synset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/nouns_and_adjectives.txt') as f:\n",
    "#     words = [line.strip() for line in f]\n",
    "# words = vocabulary\n",
    "\n",
    "words = [vocabulary[i] for i in argsort[:10000]] # use the top 10k highly-activated words from resnet50\n",
    "\n",
    "#remove single and double-letter words\n",
    "words = [w for w in words if len(w)>2] \n",
    "# singularize words while preserving word order, using the order-preserving property of python dictionaries\n",
    "words = list(dict.fromkeys([singularize(w) for w in words]).keys())\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1acf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('a'):\n",
    "    singularize(synset.name().split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb41449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefer_exact_match(synsets, word):\n",
    "    \"\"\"prefer exact match\"\"\"\n",
    "    word = singularize(word)\n",
    "    synset_singletons = [singularize(s.name().split(\".\")[0]) for s in synsets]\n",
    "    synsets_exact_match = [s for s, ss in zip(synsets, synset_singletons) if ss == word]\n",
    "    synsets_not_exact_match = [\n",
    "        s for s, ss in zip(synsets, synset_singletons) if ss != word\n",
    "    ]\n",
    "    return synsets_exact_match + synsets_not_exact_match\n",
    "\n",
    "\n",
    "def prefer_nouns(synsets, word):\n",
    "    \"\"\"prefer exact match\"\"\"\n",
    "    synsets_n = [s for s in synsets if s.name().split(\".\")[1] == \"n\"]\n",
    "    synsets_adj = [s for s in synsets if s.name().split(\".\")[1] == \"s\"]\n",
    "    synsets_other = [\n",
    "        s\n",
    "        for s in synsets\n",
    "        if s.name().split(\".\")[1] != \"n\" and s.name().split(\".\")[1] != \"s\"\n",
    "    ]\n",
    "    return synsets_n + synsets_adj + synsets_other\n",
    "\n",
    "\n",
    "def choose_synset(synsets, word):\n",
    "    synsets = prefer_exact_match(synsets, word)\n",
    "#     synsets = prefer_nouns(synsets, word)\n",
    "\n",
    "    best_synset, best_score = None, -1\n",
    "    for j, synset in enumerate(synsets):\n",
    "        lemma_names = [l.name() for l in synset.lemmas()]\n",
    "        lemma_scores = [word_frequency(ln.lower(), 'en') for ln in lemma_names]\n",
    "        ## downgrade unmatched lemma\n",
    "        lemma_scores = [score for ln, score in zip(lemma_names, lemma_scores)]\n",
    "        synset_score = np.sum(lemma_scores)\n",
    "        #         synset_score += len(lemma_names)/10 # [optional] favor synset with more lemma\n",
    "\n",
    "        #         print(list(zip(lemma_names, lemma_scores)))\n",
    "        #         print(f'[{j}]', synset.definition(), lemma_names, synset_score)\n",
    "\n",
    "        if synset_score > best_score:\n",
    "            best_score = synset_score\n",
    "            best_synset = synset\n",
    "\n",
    "    return best_synset, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_synsets = []\n",
    "word_synsets_write = []\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"word{i}\", word)\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    synsets += wn.synsets(word, pos=wn.ADJ)\n",
    "\n",
    "    if len(synsets) == 0:\n",
    "        print(f\"no synset for {word}\")\n",
    "        print(line_break)\n",
    "        continue\n",
    "\n",
    "    synset, score = choose_synset(synsets, word)\n",
    "    \n",
    "    \n",
    "    word_synsets.append(\n",
    "        [word, synset]\n",
    "    )\n",
    "    word_synsets_write.append(\n",
    "        [word, synset.name(), synset.definition()]\n",
    "    )\n",
    "    \n",
    "    print(synset)\n",
    "    print(synset.definition())\n",
    "    print(score)\n",
    "    print(line_break)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aeaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_data/wordnet.csv', 'w') as f:\n",
    "    f.write('word, synset, definition\\n')\n",
    "    for line in word_synsets_write:\n",
    "        f.write(', '.join(line)  + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71baac8d",
   "metadata": {},
   "source": [
    "## construct graph from synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfe9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b32222",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "\n",
    "for word, synset in word_synsets:\n",
    "    paths = synset.hypernym_paths()\n",
    "#     print(word)\n",
    "#     print(synset.definition())\n",
    "#     display(path)\n",
    "#     print(line_break)\n",
    "\n",
    "    for path in paths:\n",
    "        nodes = [synset.name() for synset in path]\n",
    "        graph.add_nodes_from(nodes) \n",
    "        edges = zip(nodes[1:], nodes[:-1]) # edge pointing toward more general term\n",
    "        graph.add_edges_from(edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808436f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_drawing_style = dict(\n",
    "    node_size=2,\n",
    "    width=0.5,\n",
    ")\n",
    "\n",
    "G = nx.dodecahedral_graph()\n",
    "nx.draw(\n",
    "    G,**graph_drawing_style\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, nodes in enumerate(nx.topological_generations(graph)):\n",
    "    for node in nodes:\n",
    "        graph.nodes[node][\"layer\"] = layer\n",
    "# Compute the multipartite_layout using the \"layer\" node attribute\n",
    "pos = nx.multipartite_layout(graph, subset_key=\"layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eef477",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(graph, pos=pos, **graph_drawing_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d49ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO save graph, pos to tile, visualize and refine in JS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
