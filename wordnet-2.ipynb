{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy spacy-wordnet\n",
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# !pip install wordfreq\n",
    "from wordfreq import word_frequency\n",
    "\n",
    "# sudo apt-get install mysql-client\n",
    "# sudo apt-get install mysql-server\n",
    "# sudo apt-get install libmysqlclient-dev\n",
    "# !pip install mysqlclient==2.1.1 pattern\n",
    "from pattern.en import singularize\n",
    "\n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('brown')\n",
    "# nltk.download('universal_tagset')\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")\n",
    "\n",
    "\n",
    "def word_definitions(word, return_strings=True):\n",
    "    res = [[s.name(), s.definition()] for i, s in enumerate(wn.synsets(word))]\n",
    "    if return_strings:\n",
    "        return \"\\n\".join([f\"[{name}] {definition}\" for name, definition in res])\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "def get_path(synset_name):\n",
    "    return wn.synset(synset_name).hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_definitions('objective'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d734e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_path('web.n.02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914293c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784e5c4",
   "metadata": {},
   "source": [
    "## Save imagenet synsets as file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# from natsort import natsorted\n",
    "\n",
    "# imagenet_word2synset = {}\n",
    "# fns = natsorted(glob('/home/jack/data/dataset/imagenet/val/*/'))\n",
    "# for i, fn in enumerate(fns):\n",
    "#     synset_offset = int(fn.split('/')[-2][1:])\n",
    "#     synset = wn.synset_from_pos_and_offset('n', synset_offset)\n",
    "#     word = synset.name().split('.')[0]\n",
    "# #     word += f'_{i}'\n",
    "#     imagenet_word2synset[word] = synset.name()\n",
    "    \n",
    "# with open(\"my_data/imagenet_word2synset.yaml\", \"w\") as f:\n",
    "#     yaml.dump(imagenet_word2synset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b47707",
   "metadata": {},
   "source": [
    "## Load manual word2synset correction and imagenet_word2synset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cae31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_break = \"-\" * 40\n",
    "big_line_break = \"=\" * 40\n",
    "# word_frequency = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "\n",
    "with open(\"my_data/manual_word2synset.yaml\", \"r\") as f:\n",
    "    manual_word2synset = yaml.safe_load(f)\n",
    "with open(\"my_data/imagenet_word2synset.yaml\", \"r\") as f:\n",
    "    imagenet_word2synset = yaml.safe_load(f)\n",
    "    \n",
    "# manual_word2synset, imagenet_word2synset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b98174",
   "metadata": {},
   "source": [
    "## Word list used to grab synset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/20k.txt\") as f:\n",
    "    vocabulary = [l.strip() for l in f]\n",
    "print(len(vocabulary))\n",
    "\n",
    "words = [w for w in vocabulary]\n",
    "\n",
    "\n",
    "# remove single and double-letter words\n",
    "words = [w for w in words if len(w) > 2]\n",
    "\n",
    "# singularize words while preserving word order, using the order-preserving property of python dictionaries\n",
    "# words = list(dict.fromkeys([singularize(w) for w in words]).keys())\n",
    "\n",
    "# filter by POS tag=NOUN\n",
    "# Option1 using known_tags from brown\n",
    "# known_tags = {k: v for k, v in nltk.corpus.brown.tagged_words(tagset=\"universal\")}\n",
    "# words = [w for w in words if known_tags.get(w, 'NOUN')=='NOUN']\n",
    "\n",
    "# Option2 using spacy\n",
    "# https://universaldependencies.org/u/pos/\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "words2 = []\n",
    "for w in tqdm(words):\n",
    "    tokens = nlp(w)\n",
    "    pos = [token.pos_ for token in tokens]\n",
    "    if 'NOUN' in pos or 'PROPN' in pos:\n",
    "        words2.append(w)\n",
    "words = words2\n",
    "# tokens = nlp(\" \".join(words)) ## FIXME this parse parse 'arctic_fox as'\n",
    "# words = [token.text for token in tokens if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a50e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('crab')[0].definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f96bc0",
   "metadata": {},
   "source": [
    "## word to best matching synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb41449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefer_exact_match(synsets, word):\n",
    "    \"\"\"prefer exact match\"\"\"\n",
    "    word = singularize(word)\n",
    "    synset_singletons = [singularize(s.name().split(\".\")[0]) for s in synsets]\n",
    "    synsets_exact_match = [s for s, ss in zip(synsets, synset_singletons) if ss == word]\n",
    "    synsets_not_exact_match = [\n",
    "        s for s, ss in zip(synsets, synset_singletons) if ss != word\n",
    "    ]\n",
    "    if len(synsets_exact_match) > 0:\n",
    "        return synsets_exact_match\n",
    "    else:\n",
    "        return synsets_exact_match + synsets_not_exact_match\n",
    "\n",
    "\n",
    "def prefer_nouns(synsets, word):\n",
    "    \"\"\"prefer exact match\"\"\"\n",
    "    synsets_n = [s for s in synsets if s.name().split(\".\")[1] == \"n\"]\n",
    "    synsets_adj = [s for s in synsets if s.name().split(\".\")[1] == \"s\"]\n",
    "    synsets_other = [\n",
    "        s\n",
    "        for s in synsets\n",
    "        if s.name().split(\".\")[1] != \"n\" and s.name().split(\".\")[1] != \"s\"\n",
    "    ]\n",
    "    return synsets_n + synsets_adj + synsets_other\n",
    "\n",
    "\n",
    "def choose_synset(synsets, word):\n",
    "    synsets = prefer_exact_match(synsets, word)\n",
    "    #     synsets = prefer_nouns(synsets, word)\n",
    "    \n",
    "    return synsets[0], 0\n",
    "    \n",
    "#     best_synset, best_score = None, -1\n",
    "#     #     print(big_line_break)\n",
    "#     #     print('word', word)\n",
    "#     for j, synset in enumerate(synsets):\n",
    "#         lemma_names = [l.name() for l in synset.lemmas()]\n",
    "#         lemma_scores = [word_frequency(ln.lower(), \"en\") for ln in lemma_names]\n",
    "\n",
    "#         synset_score = np.sum(lemma_scores)\n",
    "#         #         print(lemma_names)\n",
    "#         #         print(lemma_scores)\n",
    "#         #         print(line_break)\n",
    "#         if synset_score > best_score:\n",
    "#             best_score = synset_score\n",
    "#             best_synset = synset\n",
    "\n",
    "#     return best_synset, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1c45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_synsets = [] + list([[w, wn.synset(s)] for w,s in imagenet_word2synset.items()])\n",
    "\n",
    "    \n",
    "for i, word in enumerate(words):\n",
    "    print(f\"word{i}\", word)\n",
    "\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "#     synsets += wn.synsets(word, pos=wn.ADJ)\n",
    "\n",
    "    if len(synsets) == 0:\n",
    "        print(f\"no synset for {word}\")\n",
    "        print(line_break)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        if word in manual_word2synset:\n",
    "            synset = wn.synset(manual_word2synset[word])\n",
    "        else: # try the best to choose\n",
    "            synset, score = choose_synset(synsets, word)\n",
    "\n",
    "\n",
    "        word_synsets.append(\n",
    "            [word, synset]\n",
    "        )\n",
    "\n",
    "\n",
    "        print(synset)\n",
    "        print(synset.definition())\n",
    "    #     print(score)\n",
    "        print(line_break)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aeaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_synsets_write = []\n",
    "# word_synsets_write.append(\n",
    "#     [word, synset.name(), synset.definition()]\n",
    "# )\n",
    "\n",
    "# with open('my_data/wordnet.csv', 'w') as f:\n",
    "#     f.write('word,synset,definition\\n')\n",
    "#     for line in word_synsets_write:\n",
    "#         f.write(','.join(line)  + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748c1ad",
   "metadata": {},
   "source": [
    "## Construct graph\n",
    "\n",
    "- Nodes and edges are identified by synset names (e.g., \"tiger.n.02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b32222",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set(words)\n",
    "\n",
    "\n",
    "def is_concept(word):\n",
    "    return word in word_set\n",
    "\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "for word, synset in word_synsets:\n",
    "    paths = synset.hypernym_paths()\n",
    "    for path in paths:\n",
    "        path_nodes = [\n",
    "            [\n",
    "                s.name(),\n",
    "                dict(\n",
    "                    word=s.name().split(\".\")[0],\n",
    "                    synset=s.name(),\n",
    "                    definition=s.definition(),\n",
    "                    is_concept=False,\n",
    "                    hypernym_paths=None,\n",
    "                ),\n",
    "            ]\n",
    "            for s in path[:-1]\n",
    "        ]\n",
    "        graph.add_nodes_from(path_nodes)\n",
    "\n",
    "        for source, target in list(zip(path[:-1], path[1:])):\n",
    "            # convert synset object to name\n",
    "            source, target = source.name(), target.name()\n",
    "            # edge (source->target) points toward more specific terms\n",
    "            edge_id = (source, target)\n",
    "            if edge_id in graph.edges:\n",
    "                graph.edges[edge_id][\"weight\"] += 1\n",
    "            else:\n",
    "                edge_weight = 1\n",
    "                if source == 'entity.n.01' or target == 'entity.n.01':\n",
    "                    edge_weight = 10\n",
    "                graph.add_edges_from([[*edge_id, dict(weight=edge_weight)]])\n",
    "\n",
    "\n",
    "concept_nodes = []\n",
    "for word, synset in word_synsets:\n",
    "    paths = synset.hypernym_paths()\n",
    "    concept_nodes.append(\n",
    "        [\n",
    "            synset.name(),\n",
    "            dict(\n",
    "                word=word,  # Want to keep the original word to it\n",
    "                synset=synset.name(),\n",
    "                definition=synset.definition(),\n",
    "                is_concept=True,\n",
    "                hypernym_paths=[[s.name() for s in path] for path in paths],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "graph.add_nodes_from(concept_nodes)\n",
    "\n",
    "len(graph.nodes), len(graph.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66fb2e6",
   "metadata": {},
   "source": [
    "## Construct maximum_spanning_arborescence from the DAG and call it a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nx.algorithms.maximum_spanning_arborescence(graph, attr='weight', preserve_attrs=True)\n",
    "\n",
    "## sync graph node data to tree nodes\n",
    "for node_id in tree.nodes:\n",
    "    tree.nodes[node_id].update(graph.nodes[node_id])\n",
    "    \n",
    "    \n",
    "display(tree.nodes['entity.n.01'])\n",
    "print(line_break)\n",
    "display(tree.edges[('entity.n.01', 'physical_entity.n.01')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mst = nx.algorithms.maximum_spanning_tree(graph.to_undirected())\n",
    "\n",
    "# tree = nx.DiGraph()\n",
    "# tree.add_nodes_from(graph.nodes.items())\n",
    "# mst_edges = set(mst.edges())\n",
    "# tree.add_edges_from([e for e in graph.edges() if e in mst_edges or reversed(e) in mst_edges])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a49c1",
   "metadata": {},
   "source": [
    "## Compress tree nodes if the node only have 1 incoming and 1 outgoing edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in list(tree.nodes()):\n",
    "    if tree.in_degree(node) == 1 and tree.out_degree(node) == 1:\n",
    "        edges = list(tree.in_edges(node))[0], list(tree.out_edges(node))[0] \n",
    "        tree.add_edge(edges[0][0], edges[1][1])\n",
    "        tree.remove_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8352e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_graph = graph\n",
    "out_graph = tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get levels of nodes of a DAG/tree\n",
    "\n",
    "# for layer, nodes1 in enumerate(nx.topological_generations(out_graph)):\n",
    "#     for node in nodes1:\n",
    "#         out_graph.nodes[node][\"level\"] = layer\n",
    "        \n",
    "# # # Compute the multipartite_layout using the \"layer\" node attribute\n",
    "# # pos = nx.multipartite_layout(out_graph, subset_key=\"level\")\n",
    "# # for node in graph.nodes:\n",
    "# #     coord = pos[node].tolist()\n",
    "# #     out_graph.nodes[node][\"x0\"] = coord[0]\n",
    "# #     out_graph.nodes[node][\"y0\"] = coord[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66706dd6",
   "metadata": {},
   "source": [
    "## Write graph data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [node_data for node, node_data in out_graph.nodes.items()]\n",
    "for i, n in enumerate(nodes):\n",
    "    n['index'] = i # add indice to nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [edge for edge, edge_data in out_graph.edges.items()]\n",
    "\n",
    "# # compute edges in bfs order\n",
    "# roots = []\n",
    "# for node in out_graph.nodes:\n",
    "#     if out_graph.in_degree(node) == 0 and out_graph.out_degree(node) != 0:  # it's a root\n",
    "#         roots.append(node)\n",
    "# edges = [d for r in roots for d in nx.bfs_edges(out_graph, r)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89386e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9bcd4",
   "metadata": {},
   "source": [
    "### write graph data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "with open(\"my_data/wordnet_nodes.json\", \"w\") as f:\n",
    "    json.dump(nodes, f)\n",
    "\n",
    "# Edges\n",
    "with open(\"my_data/wordnet_edges.json\", \"w\") as f:\n",
    "    json.dump(edges, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc87b1b",
   "metadata": {},
   "source": [
    "### write word list from all nodes of the hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cba3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wordnet_hierarchy.txt\", \"w\") as f:\n",
    "    f.writelines([n[\"word\"] + \"\\n\" for n in nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7633b2",
   "metadata": {},
   "source": [
    "## Write hypernym paths into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roots = []\n",
    "# leaves = []\n",
    "# for node in out_graph.nodes:\n",
    "#     if out_graph.in_degree(node) == 0 and out_graph.out_degree(node) != 0:  # it's a root\n",
    "#         roots.append(node)\n",
    "# #     elif out_graph.out_degree(node) == 0 and out_graph.in_degree(node) != 0:  # it's a leaf\n",
    "#         leaves.append(node)\n",
    "\n",
    "# paths = []\n",
    "# for root in tqdm(roots):\n",
    "#     print(root)\n",
    "#     for leaf in tqdm(leaves):\n",
    "#         try:\n",
    "#             path = nx.shortest_path(out_graph, root, leaf)\n",
    "#             paths.append(dict(\n",
    "#                 path='/'.join([w.split('.')[0] for w in path]),\n",
    "#             ))\n",
    "#         except nx.NetworkXNoPath:\n",
    "#             # wrong root and leaf pair\n",
    "#             pass\n",
    "\n",
    "# with open(\"my_data/wordnet_paths.json\", \"w\") as f:\n",
    "#     json.dump(paths, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460929c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "for fn in glob(\"my_data/neuron_concept_similarities_*.npy\"):\n",
    "    a = np.load(fn)\n",
    "    np.save(fn, a.astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eef477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] drawing\n",
    "\n",
    "# graph_drawing_style = dict(\n",
    "#     node_size=2,\n",
    "#     width=0.5,\n",
    "# )\n",
    "\n",
    "# # G = nx.dodecahedral_graph()\n",
    "# # plt.figure(figsize=[2,2])\n",
    "# # nx.draw(\n",
    "# #     G,**graph_drawing_style\n",
    "# # )\n",
    "\n",
    "# # nx.draw(graph, pos=pos, **graph_drawing_style)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
