{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa9635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load(lab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# from umap import UMAP\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "\n",
    "import data_utils\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.style.use(\"seaborn-v0_8-colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1259f4",
   "metadata": {},
   "source": [
    "## get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e8d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_activation_hook(\n",
    "    cache_dict={}, key=\"test_key\", mode=None, out_device=\"cpu\"\n",
    "):\n",
    "    def hook(module, input0, output):\n",
    "        if len(output.shape) == 4:  # CNN layers\n",
    "            if mode is None or mode in [\"none\", \"None\", \"raw\"]:\n",
    "                pass  # keep output as is\n",
    "            elif mode == \"avg\":\n",
    "                output = output.mean(dim=[2, 3])\n",
    "            elif mode == \"max\":\n",
    "                output = output.amax(dim=[2, 3])\n",
    "        elif len(output.shape) == 3:  # ViT\n",
    "            output = output[:, 0].clone()\n",
    "        elif len(output.shape) == 2:  # FC layers\n",
    "            pass  # keep output as is\n",
    "        cache_dict[key] = output.to(out_device)\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "def get_activations(\n",
    "    target_model,\n",
    "    images,\n",
    "    target_layers=[\"layer4\"],\n",
    "    device=\"cuda\",\n",
    "    out_device=\"cpu\",\n",
    "    pool_mode=None,  # 'avg', 'sum' or None\n",
    "):\n",
    "    all_features = {target_layer: None for target_layer in target_layers}\n",
    "    hooks = {}\n",
    "    for target_layer in target_layers:\n",
    "        layer = eval(f\"target_model.{target_layer}\")\n",
    "        hook = layer.register_forward_hook(\n",
    "            get_cache_activation_hook(\n",
    "                cache_dict=all_features,\n",
    "                key=target_layer,\n",
    "                mode=pool_mode,\n",
    "                out_device=out_device,\n",
    "            )\n",
    "        )\n",
    "        hooks[target_layer] = hook\n",
    "\n",
    "    # Forward\n",
    "    with torch.no_grad():\n",
    "        target_model(images.to(device))\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks.values():\n",
    "        hook.remove()\n",
    "\n",
    "    # free memory\n",
    "    #     del all_features\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82429c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model, dataset and loader\n",
    "device = \"cuda\"\n",
    "target_model, target_preprocess = data_utils.get_target_model(\"resnet50\", device)\n",
    "dataset = data_utils.get_data(\"imagenet_val\", preprocess=target_preprocess)\n",
    "loader = DataLoader(dataset, batch_size=64, num_workers=8, pin_memory=True)\n",
    "\n",
    "# dataset subset\n",
    "subset = list(range(0, len(dataset), 10))\n",
    "sub_dataset = torch.utils.data.Subset(dataset, subset)\n",
    "sub_loader = DataLoader(sub_dataset, batch_size=256, num_workers=8, pin_memory=True)\n",
    "\n",
    "target_layers = [\"layer3\", \"layer4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb460b6c",
   "metadata": {},
   "source": [
    "## Estimate quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f813bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 0.95\n",
    "quantile_samples = {layer_name: [] for layer_name in target_layers}\n",
    "\n",
    "for images, labels in tqdm(sub_loader):\n",
    "    # grab mini-batch activations for all target layers in a dictionary\n",
    "    features = get_activations(\n",
    "        target_model,\n",
    "        images.to(device),\n",
    "        target_layers=target_layers,\n",
    "    )\n",
    "\n",
    "    # gien a minibatch, for each layer, estimate channel-wise activation quantile\n",
    "    for layer_name, act in features.items():\n",
    "        if len(act.shape) == 4:  # Conv layer\n",
    "            # channel first, then combine all remaining (spatial, and instance) dimensions\n",
    "            act1 = act.permute(1, 0, 2, 3).reshape(act.shape[1], -1)\n",
    "        elif len(act.shape) == 2:  # fc layer\n",
    "            # neuron first\n",
    "            act1 = act.permute(1, 0)\n",
    "        q = np.quantile(act1.numpy(), q=quantile, axis=1)\n",
    "        quantile_samples[layer_name].append(q)\n",
    "\n",
    "for layer_name, quantile_sample in quantile_samples.items():\n",
    "    quantile_samples[layer_name] = np.stack(quantile_sample)\n",
    "\n",
    "# On each layer, take mean of quantile samples as final quantile estimate,\n",
    "quantiles = {}\n",
    "for layer_name, quantile_sample in quantile_samples.items():\n",
    "    quantiles[layer_name] = quantile_sample.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d6c0",
   "metadata": {},
   "source": [
    "## Cross-layer neuron-to-neuron IoUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4744df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neuron-to-neuron IoUs FOR EVERY PAIRS of layers\n",
    "sum_intersections = {}\n",
    "sum_unions = {}\n",
    "ious = {}\n",
    "for images, labels in tqdm(sub_loader):\n",
    "    features = get_activations(\n",
    "        target_model,\n",
    "        images.to(device),\n",
    "        target_layers=target_layers,\n",
    "    )\n",
    "\n",
    "    # WIP: take every pairs of layers\n",
    "    for i, layer_name1 in enumerate(target_layers):\n",
    "        for layer_name2 in target_layers[i:]:\n",
    "            act1 = features[layer_name1]\n",
    "            act2 = features[layer_name2]\n",
    "\n",
    "            # WIP upsample act2, if act2 smaller than act1\n",
    "            if act1.shape[2] < act2.shape[2]:\n",
    "                upsample = torch.nn.Upsample(size=act2.shape[2])\n",
    "                act1 = upsample(act1)\n",
    "            elif act1.shape[2] > act2.shape[2]:\n",
    "                upsample = torch.nn.Upsample(size=act1.shape[2])\n",
    "                act2 = upsample(act2)\n",
    "\n",
    "            n_channel1 = act1.shape[1]\n",
    "            threshold1 = quantiles[layer_name1]\n",
    "            threshold1 = torch.from_numpy(threshold1).view(1, threshold1.shape[0], 1, 1)\n",
    "            act_mask1 = (act1 > threshold1).float()\n",
    "            channel_first_act_mask1 = act_mask1.permute(1, 0, 2, 3).reshape(\n",
    "                n_channel1, -1\n",
    "            )\n",
    "\n",
    "            n_channel2 = act2.shape[1]\n",
    "            threshold2 = quantiles[layer_name2]\n",
    "            threshold2 = torch.from_numpy(threshold2).view(1, threshold2.shape[0], 1, 1)\n",
    "            act_mask2 = (act2 > threshold2).float()\n",
    "            channel_first_act_mask2 = act_mask2.permute(1, 0, 2, 3).reshape(\n",
    "                n_channel2, -1\n",
    "            )\n",
    "\n",
    "            key = f\"{layer_name1},{layer_name2}\"\n",
    "            key_inv = f\"{layer_name2},{layer_name1}\"\n",
    "            if key not in sum_intersections:\n",
    "                sum_intersections[key] = torch.zeros(n_channel1, n_channel2)\n",
    "            if key not in sum_unions:\n",
    "                sum_unions[key] = torch.zeros(n_channel1, n_channel2)\n",
    "            if key_inv not in sum_intersections:\n",
    "                sum_intersections[key_inv] = torch.zeros(n_channel2, n_channel1)\n",
    "            if key_inv not in sum_unions:\n",
    "                sum_unions[key_inv] = torch.zeros(n_channel2, n_channel1)\n",
    "\n",
    "            # (sum of) intersections\n",
    "            sum_intersections[key] += (\n",
    "                channel_first_act_mask1 @ channel_first_act_mask2.t()\n",
    "            )\n",
    "            if key != key_inv:  # different layer\n",
    "                sum_intersections[key_inv] = sum_intersections[key].t()\n",
    "\n",
    "            # (sum of) unions\n",
    "            # for each channel pair, grab all activation masks and compute union\n",
    "\n",
    "            if layer_name1 == layer_name2:  # same layer, result will be symmetric\n",
    "                for i in tqdm(range(n_channel1)):\n",
    "                    for j in range(i + 1, n_channel2):\n",
    "                        ci = channel_first_act_mask1[i]\n",
    "                        cj = channel_first_act_mask2[j]\n",
    "                        union_ij_batch = ((ci + cj) > 0).sum()\n",
    "                        sum_unions[key][i, j] += union_ij_batch\n",
    "                        # same layer, matrix is symmetric\n",
    "                        sum_unions[key][j, i] += union_ij_batch\n",
    "\n",
    "            else:  # different layers\n",
    "                for i in tqdm(range(n_channel1)):\n",
    "                    for j in range(n_channel2):\n",
    "                        ci = channel_first_act_mask1[i]\n",
    "                        cj = channel_first_act_mask2[j]\n",
    "                        union_ij_batch = ((ci + cj) > 0).sum()\n",
    "                        sum_unions[key][i, j] += union_ij_batch\n",
    "                        sum_unions[key_inv][j, i] += union_ij_batch\n",
    "\n",
    "ious = {\n",
    "    key: sum_intersections[key] / sum_unions[key]\n",
    "    for key in sum_intersections.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990cf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = {\n",
    "    key: sum_intersections[key] / sum_unions[key]\n",
    "    for key in sum_intersections.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fae302",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    dict(\n",
    "        sum_intersections=sum_intersections,\n",
    "        sum_unions=sum_unions,\n",
    "        ious=ious,\n",
    "    ),\n",
    "    \"my_data/iou.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a08bd",
   "metadata": {},
   "source": [
    "## Per-layer neuron-to-neuron IoUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_intersections = {}\n",
    "# sum_unions = {}\n",
    "# ious = {}\n",
    "# for images, labels in tqdm(sub_loader):\n",
    "#     features = get_activations(\n",
    "#         target_model,\n",
    "#         images.to(device),\n",
    "#         target_layers=target_layers,\n",
    "#     )\n",
    "    \n",
    "#     for layer_name, act in features.items():\n",
    "#         threshold = quantiles[layer_name]\n",
    "#         threshold = torch.from_numpy(threshold).view(1, threshold.shape[0], 1, 1)\n",
    "\n",
    "#         # activation mask\n",
    "#         n_instance = act.shape[0]\n",
    "#         n_channel = act.shape[1]\n",
    "#         if layer_name not in sum_intersections:\n",
    "#             sum_intersections[layer_name] = torch.zeros(n_channel, n_channel)\n",
    "#         if layer_name not in sum_unions:\n",
    "#             sum_unions[layer_name] = torch.zeros(n_channel, n_channel)\n",
    "\n",
    "#         act_mask = (act > threshold).float()\n",
    "#         channel_first_act_mask = act_mask.permute(1, 0, 2, 3).reshape(n_channel, -1)\n",
    "#         sum_intersections[layer_name] += (\n",
    "#             channel_first_act_mask @ channel_first_act_mask.t()\n",
    "#         )\n",
    "\n",
    "#         # for each channel pair, grab all activation masks and compute union\n",
    "#         for i in tqdm(range(n_channel)):\n",
    "#             for j in range(i, n_channel):\n",
    "#                 ci = channel_first_act_mask[i]\n",
    "#                 cj = channel_first_act_mask[j]\n",
    "#                 union_ij_batch = ((ci + cj) > 0).sum()\n",
    "#                 sum_unions[layer_name][i, j] += union_ij_batch\n",
    "#                 sum_unions[layer_name][j, i] += union_ij_batch\n",
    "\n",
    "# ious = {\n",
    "#     layer_name: sum_intersections[layer_name] / sum_unions[layer_name]\n",
    "#     for layer_name in sum_intersections.keys()\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     dict(\n",
    "#         sum_intersections=sum_intersections,\n",
    "#         sum_unions=sum_unions,\n",
    "#         ious=ious,\n",
    "#     ),\n",
    "#     \"my_data/iou.pt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ious[\"layer3\"][np.arange(1024), np.arange(1024)] = 1\n",
    "# ious[\"layer4\"][np.arange(2048), np.arange(2048)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70080350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import SpectralCoclustering\n",
    "\n",
    "# sc = SpectralCoclustering(n_clusters=10, n_init=20)\n",
    "# sc.fit(ious[\"layer3\"])\n",
    "# col_order = np.argsort(sc.column_labels_)\n",
    "# row_order = np.argsort(sc.row_labels_)\n",
    "\n",
    "# iou = ious[\"layer3\"].clone()\n",
    "# iou[np.arange(iou.shape[0]), np.arange(iou.shape[0])] = 0\n",
    "\n",
    "# plt.figure(figsize=[12, 12])\n",
    "# plt.imshow(iou[row_order][:, col_order])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334896f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "\n",
    "# for layer_name, iou in ious.items():\n",
    "#     umap = UMAP(metric=\"precomputed\").fit_transform(1 - iou)\n",
    "#     plt.scatter(umap[:, 0], umap[:, 1], s=12)\n",
    "#     plt.title(layer_name)\n",
    "#     plt.show()\n",
    "\n",
    "#     np.save(f\"my_data/umap_from_iou_{layer_name}.npy\", umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3815dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d4274a",
   "metadata": {},
   "source": [
    "## Old\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after:\n",
    "# !python save_activations.py\n",
    "# saved_activations/ has activation files:\n",
    "# ./saved_activations/raw-{layer_name}-{batch_start}.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b69fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load activations files, estimate quantile\n",
    "# layer_name = \"layer4\"\n",
    "# fns = natsorted(glob(f\"./saved_activations/raw-{layer_name}-*.pt\"))\n",
    "# display(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2bd3f",
   "metadata": {},
   "source": [
    "## Load minibatch activation files, compute quantile per neuron, on each mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile = 0.95\n",
    "\n",
    "# quantile_samples = []\n",
    "# for fn in tqdm(fns):\n",
    "#     act = torch.load(fn)\n",
    "\n",
    "#     if len(act.shape) == 4:  # Conv layer\n",
    "#         # channel first, then combine all remaining (spatial, and instance) dimensions\n",
    "#         act1 = act.permute(1, 0, 2, 3).reshape(act.shape[1], -1)\n",
    "#     elif len(act.shape) == 2:  # fc layer\n",
    "#         # neuron first\n",
    "#         act1 = act.permute(1, 0)\n",
    "\n",
    "#     q = np.quantile(act1.numpy(), q=quantile, axis=1)\n",
    "#     quantile_samples.append(q)\n",
    "\n",
    "# quantile_samples = np.stack(quantile_samples)\n",
    "\n",
    "# plt.stem(quantile_samples[:, 0])\n",
    "# quantile_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quantile_estimates per neuron (channel)\n",
    "# quantile_estimates = np.mean(quantile_samples, axis=0)\n",
    "# quantile_estimates.shape, quantile_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cb30d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d59226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = torch.from_numpy(quantile_estimates).view(\n",
    "#     1, quantile_estimates.shape[0], 1, 1\n",
    "# )\n",
    "\n",
    "# sum_intersection = None\n",
    "# sum_union = None\n",
    "# # activation mask\n",
    "\n",
    "\n",
    "# for fn in tqdm(fns):\n",
    "#     act = torch.load(fn)\n",
    "#     #TODO: instead of load activation from file, compute it on the fly from images\n",
    "    \n",
    "#     n_instance = act.shape[0]\n",
    "#     n_channel = act.shape[1]\n",
    "#     if sum_intersection is None:\n",
    "#         sum_itersection = torch.zeros(n_channel, n_channel)\n",
    "#         sum_union = torch.zeros(n_channel, n_channel)\n",
    "\n",
    "#     act_mask = (act > threshold).float()\n",
    "#     channel_first_act_mask = act_mask.permute(1, 0, 2, 3).reshape(n_channel, -1)\n",
    "#     sum_itersection += channel_first_act_mask @ channel_first_act_mask.t()\n",
    "\n",
    "#     # for each channel pair, grab all activation masks and compute union\n",
    "#     for i in tqdm(range(n_channel)):\n",
    "#         for j in range(n_channel):\n",
    "#             ci = channel_first_act_mask[i]\n",
    "#             cj = channel_first_act_mask[j]\n",
    "#             sum_union[i, j] = ((ci + cj) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou = sum_intersection / sum_union\n",
    "# np.save(f'my_data/iou_{layer_name}.npy', iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, [channel_threshold, channel_act] in enumerate(\n",
    "#     zip(tqdm(quantile_estimates), act.permute(1, 0, 2, 3))\n",
    "# ):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e66b30",
   "metadata": {},
   "source": [
    "## visualize binary masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a61aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, [channel_threshold, channel_act] in enumerate(\n",
    "#     zip(quantile_estimates, act.permute(1, 0, 2, 3))\n",
    "# ):\n",
    "#     act_mask = channel_act > channel_threshold\n",
    "#     nrow = 16\n",
    "#     ncol = math.ceil(act_mask.shape[0] / nrow)\n",
    "#     grid = make_grid(act_mask.unsqueeze(1), nrow=nrow, padding=0)[0]\n",
    "#     plt.figure(figsize=[12, 6])\n",
    "#     plt.imshow(grid)\n",
    "#     #     plt.axis(\"off\")\n",
    "#     plt.xticks(np.linspace(0, grid.shape[1], nrow + 1) - 0.5)\n",
    "#     plt.yticks(np.linspace(0, grid.shape[0], ncol + 1) - 0.5)\n",
    "#     plt.show()\n",
    "#     if i > 2:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fn in tqdm(fns):\n",
    "#     act = torch.load(fn)\n",
    "\n",
    "#     if len(act.shape) == 4:  # Conv layer\n",
    "#         # channel first, then combine all remaining (spatial, and instance) dimensions\n",
    "#         act1 = act.permute(1, 0, 2, 3).reshape(act.shape[1], -1)\n",
    "#     elif len(act.shape) == 2:  # fc layer\n",
    "#         # neuron first\n",
    "#         act1 = act.permute(1, 0)\n",
    "\n",
    "#     q = np.quantile(act1.numpy(), q=quantile, axis=1)\n",
    "#     quantile_samples.append(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e8fc1",
   "metadata": {},
   "source": [
    "## visualize original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get dataset\n",
    "\n",
    "# target_model, target_preprocess = data_utils.get_target_model(\"resnet50\", \"cpu\")\n",
    "# dataset = data_utils.get_data(\"imagenet_val\", preprocess=None)\n",
    "\n",
    "# # subset\n",
    "# subset = list(range(0, len(dataset), 10))\n",
    "# dataset = torch.utils.data.Subset(dataset, subset)\n",
    "\n",
    "# target_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# imgs = torch.empty(136, 3, 224, 224)\n",
    "# for i, img_index in enumerate(dataset.indices[-136:]):\n",
    "#     img = dataset.dataset[img_index][0]\n",
    "#     img = transform(img)\n",
    "#     imgs[i] = img\n",
    "# #     display(img)\n",
    "# #     plt.imshow(img.permute(1, 2, 0).numpy())\n",
    "\n",
    "# grid = make_grid(imgs, nrow=16)\n",
    "# plt.figure(figsize=[12, 6])\n",
    "# plt.imshow(\n",
    "#     grid.permute(1, 2, 0).numpy(),\n",
    "# )\n",
    "# plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb2260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
